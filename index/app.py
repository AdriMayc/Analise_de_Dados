#%% 
import pandas as pd
import plotly.express as px
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import streamlit as st
import os
from datetime import datetime

# Configura√ß√£o da p√°gina - Layout amplo (sem barra lateral)
st.set_page_config(
    page_title="An√°lise de E-commerce Olist",
    layout="wide"
)

# T√≠tulo principal
st.title("An√°lise de Dados do E-commerce Brasileiro - Olist")
st.markdown("""
### Sobre o Projeto

Este dashboard analisa dados reais do e-commerce brasileiro **Olist**, com foco em tr√™s pilares:

- **Comportamento do cliente**: padr√µes de compra, segmenta√ß√£o via RFM, churn e avalia√ß√£o de pedidos.
- **Efici√™ncia log√≠stica**: an√°lise de atrasos na entrega e impacto na satisfa√ß√£o.
- **Insights de neg√≥cio**: identifica√ß√£o de oportunidades regionais e recomenda√ß√µes estrat√©gicas.

O projeto come√ßa com o **carregamento e tratamento completo dos dados**, passando por convers√£o de datas, tradu√ß√£o de categorias, preenchimento de aus√™ncias e c√°lculo de m√©tricas log√≠sticas.

A seguir, s√£o exploradas **visualiza√ß√µes interativas** que revelam rela√ß√µes entre log√≠stica, avalia√ß√£o e comportamento de compra. Por fim, s√£o apresentadas **recomenda√ß√µes pr√°ticas** para melhorar a experi√™ncia do cliente e a performance da opera√ß√£o.

""")

st.markdown("---")

# Fun√ß√£o para carregar os dados
def carregar_dados():
    """Carrega todos os arquivos de dados do conjunto Olist"""
    try:
        data_path = "data/"
        
        customers = pd.read_csv(data_path + "olist_customers_dataset.csv")
        geolocation = pd.read_csv(data_path + "olist_geolocation_dataset.csv")
        order_items = pd.read_csv(data_path + "olist_order_items_dataset.csv")
        order_payments = pd.read_csv(data_path + "olist_order_payments_dataset.csv")
        order_reviews = pd.read_csv(data_path + "olist_order_reviews_dataset.csv")
        orders = pd.read_csv(data_path + "olist_orders_dataset.csv")
        products = pd.read_csv(data_path + "olist_products_dataset.csv")
        sellers = pd.read_csv(data_path + "olist_sellers_dataset.csv")
        product_category = pd.read_csv(data_path + "product_category_name_translation.csv")
        
        
        return {
            "customers": customers,
            "geolocation": geolocation, 
            "order_items": order_items,
            "order_payments": order_payments,
            "order_reviews": order_reviews,
            "orders": orders,
            "products": products,
            "sellers": sellers,
            "product_category": product_category
        }
    
    except FileNotFoundError:
        st.error("Arquivos n√£o encontrados. Verifique se os arquivos est√£o na pasta 'data'.")
        return None

# Fun√ß√£o para fazer o tratamento inicial dos dados
def tratar_dados(dataframes):
    """Realiza o tratamento inicial dos dados"""
    
    # Criando colunas para organizar o conte√∫do da esquerda para a direita
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("1. Convers√£o de Datas")
        # Convertendo colunas de data para o formato datetime
        if 'orders' in dataframes:
            date_columns = ['order_purchase_timestamp', 'order_approved_at', 
                          'order_delivered_carrier_date', 'order_delivered_customer_date',
                          'order_estimated_delivery_date']
            
            for col in date_columns:
                dataframes['orders'][col] = pd.to_datetime(dataframes['orders'][col], errors='coerce')
            
            st.write("‚úÖ Datas em tabela 'orders' convertidas para datetime")
        
        if 'order_reviews' in dataframes:
            dataframes['order_reviews']['review_creation_date'] = pd.to_datetime(dataframes['order_reviews']['review_creation_date'], errors='coerce')
            dataframes['order_reviews']['review_answer_timestamp'] = pd.to_datetime(dataframes['order_reviews']['review_answer_timestamp'], errors='coerce')
            st.write("‚úÖ Datas em tabela 'order_reviews' convertidas para datetime")
    
    with col2:
        st.subheader("2. Tradu√ß√£o de Categorias")
        # Traduzindo nomes das categorias de produtos
        if 'products' in dataframes and 'product_category' in dataframes:
            # Mostrar amostra antes da tradu√ß√£o
            st.write("Antes da tradu√ß√£o (amostra):")
            st.dataframe(dataframes['products'][['product_id', 'product_category_name']].head(3))
            
            # Merge para traduzir os nomes das categorias
            products_with_category = dataframes['products'].merge(
                dataframes['product_category'], 
                on='product_category_name', 
                how='left'
            )
            
            # Substituir o DataFrame original
            dataframes['products'] = products_with_category
            
            # Mostrar amostra ap√≥s a tradu√ß√£o
            st.write("Ap√≥s a tradu√ß√£o (amostra):")
            st.dataframe(dataframes['products'][['product_id', 'product_category_name', 'product_category_name_english']].head(3))
    
    st.markdown("---")
    
    # Segunda linha de colunas
    col3, col4 = st.columns(2)
    
    with col3:
        st.subheader("3. C√°lculo de M√©tricas de Entrega")
        # Calcular m√©tricas relacionadas a entregas
        if 'orders' in dataframes:
            # Tempo de entrega (em dias)
            dataframes['orders']['delivery_time'] = (
                dataframes['orders']['order_delivered_customer_date'] - 
                dataframes['orders']['order_purchase_timestamp']
            ).dt.days
            
            # Se a entrega atrasou
            dataframes['orders']['delayed_delivery'] = dataframes['orders']['order_delivered_customer_date'] > dataframes['orders']['order_estimated_delivery_date']
            
            # Status de entrega simplificado
            status_map = {
                'delivered': 'entregue',
                'shipped': 'enviado',
                'canceled': 'cancelado',
                'unavailable': 'indispon√≠vel',
                'invoiced': 'faturado',
                'processing': 'processando',
                'approved': 'aprovado',
                'created': 'criado'
            }
            dataframes['orders']['order_status_pt'] = dataframes['orders']['order_status'].map(status_map)
            
            # Mostrar amostra ap√≥s os c√°lculos
            st.write("Novas colunas criadas:")
            st.dataframe(dataframes['orders'][['order_id', 'delivery_time', 'delayed_delivery', 'order_status', 'order_status_pt']].head(5))
    
    with col4:
        st.subheader("4. Tratamento de Valores Ausentes")
        # Tratando valores ausentes nos produtos
        if 'products' in dataframes:
            # Contagem de valores ausentes antes do tratamento
            missing_before = dataframes['products'][['product_weight_g', 'product_length_cm', 'product_height_cm', 'product_width_cm']].isna().sum()
            st.write("Valores ausentes antes do tratamento:")
            st.write(missing_before)
            
            # Preenchendo valores ausentes nas dimens√µes e peso com a mediana de sua categoria
            for col in ['product_weight_g', 'product_length_cm', 'product_height_cm', 'product_width_cm']:
                # Agrupar por categoria e calcular a mediana
                median_by_category = dataframes['products'].groupby('product_category_name')[col].transform('median')
                # Preencher NaN com a mediana da categoria
                dataframes['products'][col] = dataframes['products'][col].fillna(median_by_category)
                # Se ainda houver NaN (categorias sem valores), preencher com a mediana geral
                dataframes['products'][col] = dataframes['products'][col].fillna(dataframes['products'][col].median())
            
            # Contagem de valores ausentes ap√≥s o tratamento
            missing_after = dataframes['products'][['product_weight_g', 'product_length_cm', 'product_height_cm', 'product_width_cm']].isna().sum()
            st.write("Valores ausentes ap√≥s o tratamento:")
            st.write(missing_after)
    
    st.markdown("---")
    
    # Terceira linha de colunas
    col5, col6 = st.columns(2)
    
    with col5:
        st.subheader("5. Tratamento de Avalia√ß√µes")
        # Tratando valores ausentes nos reviews
        if 'order_reviews' in dataframes:
            # Estat√≠sticas antes do tratamento
            review_stats_before = {
                "Coment√°rios ausentes": dataframes['order_reviews']['review_comment_message'].isna().sum(),
                "Scores ausentes": dataframes['order_reviews']['review_score'].isna().sum()
            }
            st.write("Antes do tratamento:")
            st.write(review_stats_before)
            
            # Preencher coment√°rios vazios
            dataframes['order_reviews']['review_comment_message'] = dataframes['order_reviews']['review_comment_message'].fillna('Sem coment√°rio')
            # Converter score para int (ap√≥s tratar poss√≠veis NaN)
            dataframes['order_reviews']['review_score'] = dataframes['order_reviews']['review_score'].fillna(0).astype(int)
            
            # Estat√≠sticas ap√≥s o tratamento
            review_stats_after = {
                "Coment√°rios ausentes": dataframes['order_reviews']['review_comment_message'].isna().sum(),
                "Scores ausentes": dataframes['order_reviews']['review_score'].isna().sum()
            }
            st.write("Ap√≥s o tratamento:")
            st.write(review_stats_after)
    
    with col6:
        st.subheader("6. Remo√ß√£o de Duplicatas")
        # Verificar e remover duplicatas
        duplicates_info = {}
        duplicates_removed = {}
        
        for name, df in dataframes.items():
            # Contar duplicatas antes da remo√ß√£o
            duplicates = df.duplicated().sum()
            duplicates_info[name] = duplicates
            
            # Remover duplicatas se existirem
            if duplicates > 0:
                rows_before = df.shape[0]
                dataframes[name] = df.drop_duplicates(keep='first')
                rows_after = dataframes[name].shape[0]
                duplicates_removed[name] = rows_before - rows_after
        
        # Exibir informa√ß√µes sobre duplicatas
        st.write("Duplicatas encontradas por tabela:")
        for name, count in duplicates_info.items():
            if count > 0:
                st.write(f"‚Ä¢ {name}: {count} duplicatas encontradas e removidas")
            else:
                st.write(f"‚Ä¢ {name}: nenhuma duplicata")
        
        # Resumo da remo√ß√£o
        if sum(duplicates_info.values()) > 0:
            st.success(f"Total de {sum(duplicates_info.values())} duplicatas foram encontradas e removidas com sucesso.")
        else:
            st.success("N√£o foram encontradas duplicatas nos dados.")
    
    st.markdown("---")
    st.subheader("Resumo do Tratamento de Dados")
    
    # Resumo do tratamento
    col7, col8 = st.columns(2)
    
    with col7:
        # Exibir dimens√µes de cada dataframe
        st.write("Dimens√µes dos Dados:")
        dimensions = {}
        for name, df in dataframes.items():
            dimensions[name] = df.shape
        
        for name, dim in dimensions.items():
            st.write(f"‚Ä¢ {name}: {dim[0]:,} linhas √ó {dim[1]} colunas")
    
    with col8:
        # Exibir informa√ß√µes sobre tipos de dados
        st.write("Tipos de Dados (amostra para orders):")
        if 'orders' in dataframes:
            st.write(dataframes['orders'].dtypes)
    
    return dataframes

def main():
    st.header("Carregamento e Tratamento dos Dados Olist")
    
    data = carregar_dados()
    
    if data:
        # Processar os dados
        data_tratados = tratar_dados(data)
        
        # Salvar os dados no estado da sess√£o (para uso posterior)
        st.session_state['data'] = data_tratados
        
        st.success("Tratamento de dados conclu√≠do com sucesso!")
        
        # Exibir cabe√ßalho de uma tabela tratada como exemplo
        st.subheader("Amostra dos Dados Tratados (Orders)")
        st.dataframe(data_tratados['orders'].head())

if __name__ == "__main__":
    main()

st.divider()

#%%
dados = carregar_dados()
orders = dados["orders"]
reviews = dados["order_reviews"]

df = pd.merge(orders, reviews[["order_id", "review_score"]], on="order_id", how="inner")
df["atraso_entrega"] = df["order_delivered_customer_date"] > df["order_estimated_delivery_date"]
df["atraso_entrega"] = df["atraso_entrega"].fillna(False)
df["status_entrega"] = df["atraso_entrega"].map({True: "Com Atraso", False: "Sem Atraso"})

# T√≠tulo e explica√ß√£o
st.markdown("""
### Rela√ß√£o entre Atraso na Entrega e Avalia√ß√£o do Cliente

Visualiza√ß√£o da distribui√ß√£o das notas de avalia√ß√£o conforme o status da entrega. A hip√≥tese √© que entregas com atraso tendem a resultar em notas mais baixas.
""")

# Gr√°fico Plotly
fig = px.box(
    df,
    x="status_entrega",
    y="review_score",
    color="status_entrega",
    points=False,
    title="<b>Avalia√ß√µes de Clientes por Status de Entrega</b>",
    labels={
        "status_entrega": "Entrega",
        "review_score": "Nota de Avalia√ß√£o"
    },
    width=1000,
    height=500,
    color_discrete_sequence=["#00cc96", "#EF553B"]  # Verde para sem atraso, vermelho para com
)

fig.update_layout(
    plot_bgcolor='rgba(0,0,0,0)',
    paper_bgcolor='rgba(0,0,0,0)',
    showlegend=False,
    margin=dict(l=50, r=50, t=80, b=50),
    font=dict(color='white')  # Fica melhor com tema escuro
)

st.plotly_chart(fig, use_container_width=True)

# C√°lculo das m√©dias
media_sem_atraso = df[df["atraso_entrega"] == False]["review_score"].mean()
media_com_atraso = df[df["atraso_entrega"] == True]["review_score"].mean()

# Exibir valores para insights
st.markdown("### M√©dias das Avalia√ß√µes:")
st.markdown(f"- üü© **Sem atraso:** {media_sem_atraso:.2f}")
st.markdown(f"- üü• **Com atraso:** {media_com_atraso:.2f}")

st.divider()

#%%

# 1. Juntar datasets necess√°rios
df_reclamacoes = (
    dados['order_items']
    .merge(dados['products'], on='product_id')
    .merge(dados['order_reviews'], on='order_id')
    .merge(dados['product_category'], on='product_category_name')
)

# 2. Criar flag de reclama√ß√£o (notas ruins)
df_reclamacoes['reclamacao'] = df_reclamacoes['review_score'] <= 2  # 1 ou 2

# 3. Agrupar por categoria traduzida
reclamacoes_por_categoria = (
    df_reclamacoes
    .groupby('product_category_name_english')
    .agg(total_pedidos=('order_id', 'count'),
         total_reclamacoes=('reclamacao', 'sum'))
    .reset_index()
)

# 4. Calcular taxa de reclama√ß√£o (%)
reclamacoes_por_categoria['taxa_reclamacao'] = (
    reclamacoes_por_categoria['total_reclamacoes'] / reclamacoes_por_categoria['total_pedidos']
) * 100

# 5. Filtrar categorias com mais de 100 pedidos (para evitar distor√ß√µes)
reclamacoes_filtrado = reclamacoes_por_categoria[reclamacoes_por_categoria['total_pedidos'] > 100]

# 6. Top 10 com maior taxa de reclama√ß√£o
top10_reclamacoes = (
    reclamacoes_filtrado
    .sort_values(by='taxa_reclamacao', ascending=False)
    .head(10)
)


# Gr√°fico
fig = px.bar(
    top10_reclamacoes,
    x='taxa_reclamacao',
    y='product_category_name_english',
    orientation='h',
    text=top10_reclamacoes['taxa_reclamacao'].apply(lambda x: f'{x:.1f}%'),
    title='<b>Top 10 Categorias com Maior Taxa de Reclama√ß√µes</b>',
    labels={
        'product_category_name_english': 'Categoria',
        'taxa_reclamacao': 'Taxa de Reclama√ß√£o (%)'
    },
    color='taxa_reclamacao',
    color_continuous_scale='Blues',
    height=500
)

fig.update_layout(
    plot_bgcolor='rgba(0,0,0,0)',
    paper_bgcolor='rgba(0,0,0,0)',
    font=dict(color='white'),
    margin=dict(l=50, r=50, t=80, b=50)
)

st.plotly_chart(fig, use_container_width=True)

st.markdown("""
*Esta visualiza√ß√£o mostra as **categorias com maior taxa de reclama√ß√µes**, 
baseando-se em avalia√ß√µes com nota 1 ou 2. 
Apenas categorias com mais de 100 pedidos foram consideradas, para garantir relev√¢ncia estat√≠stica.*
""")

def gerar_texto_reclamacoes(df_top):
    """Gera texto descritivo com base no top 10 de categorias com maior taxa de reclama√ß√£o"""
    texto = ""

    df_top = df_top.sort_values(by='taxa_reclamacao', ascending=False).reset_index(drop=True)

    cat_top1 = df_top.loc[0, 'product_category_name_english']
    taxa_top1 = df_top.loc[0, 'taxa_reclamacao']
    total_top1 = df_top.loc[0, 'total_pedidos']

    cat_top2 = df_top.loc[1, 'product_category_name_english']
    taxa_top2 = df_top.loc[1, 'taxa_reclamacao']

    cat_top3 = df_top.loc[2, 'product_category_name_english']
    taxa_top3 = df_top.loc[2, 'taxa_reclamacao']

    media_top10 = df_top['taxa_reclamacao'].mean()

    texto = "### An√°lise das Categorias com Maior Taxa de Reclama√ß√µes\n\n"
    texto += f"- A categoria com maior taxa de reclama√ß√µes √© **{cat_top1}**, com **{taxa_top1:.1f}%** dos pedidos resultando em avalia√ß√µes negativas (nota 1 ou 2), considerando um total de {total_top1} pedidos.\n\n"
    texto += f"- Em seguida, destacam-se **{cat_top2}** com **{taxa_top2:.1f}%** e **{cat_top3}** com **{taxa_top3:.1f}%**.\n\n"
    texto += f"- A m√©dia de reclama√ß√µes entre as 10 categorias com pior desempenho √© de **{media_top10:.1f}%**.\n\n"
    texto += f"- Esses dados indicam poss√≠veis pontos cr√≠ticos nessas categorias, que podem estar relacionados √† qualidade dos produtos, log√≠stica ou experi√™ncia do cliente. Recomenda-se uma an√°lise mais detalhada para identifica√ß√£o das causas e defini√ß√£o de a√ß√µes corretivas.\n"

    return texto


st.markdown(gerar_texto_reclamacoes(top10_reclamacoes))

st.divider()

#%%

# 1. Juntar datasets necess√°rios
df_correlacao = (
    dados['order_items']
    .merge(dados['order_reviews'], on='order_id')
    .merge(dados['products'], on='product_id')
)

# 2. Calcular a m√©dia das avalia√ß√µes por produto
df_media_reviews = df_correlacao.groupby('product_id').agg(
    media_avaliacao=('review_score', 'mean'),
    preco=('price', 'mean')
).reset_index()

# 3. Gerar gr√°fico de dispers√£o para visualizar a correla√ß√£o
fig = px.scatter(
    df_media_reviews,
    x='preco',
    y='media_avaliacao',
    title='<b>Correla√ß√£o entre Pre√ßo do Produto e Nota M√©dia</b>',
    labels={'preco': 'Pre√ßo (R$)', 'media_avaliacao': 'Nota M√©dia de Avalia√ß√£o'},
    color='media_avaliacao',
    color_continuous_scale='Greens',
    height=500
)

# 4. Ajustes no layout para gr√°fico bonito e clean
fig.update_layout(
    plot_bgcolor='rgba(0,0,0,0)',
    paper_bgcolor='rgba(0,0,0,0)',
    font=dict(color='white'),
    margin=dict(l=50, r=50, t=80, b=50)
)

# Exibir gr√°fico
st.plotly_chart(fig, use_container_width=True)

# Calcular insights num√©ricos
correlacao = df_media_reviews['preco'].corr(df_media_reviews['media_avaliacao'])
preco_medio_5 = df_media_reviews[df_media_reviews['media_avaliacao'] == 5]['preco'].mean()
preco_medio_1 = df_media_reviews[df_media_reviews['media_avaliacao'] == 1]['preco'].mean()
preco_mais_bem_avaliado = df_media_reviews.sort_values('media_avaliacao', ascending=False).iloc[0]
preco_mais_mal_avaliado = df_media_reviews.sort_values('media_avaliacao').iloc[0]

# Texto com an√°lise
st.markdown("### An√°lise da Correla√ß√£o entre Pre√ßo e Avalia√ß√£o")
st.markdown(f"""
- A correla√ß√£o entre **pre√ßo** e **nota m√©dia de avalia√ß√£o** √© de **{correlacao:.2f}**, indicando uma rela√ß√£o {"positiva" if correlacao > 0 else "negativa" if correlacao < 0 else "nula"}.
- Produtos com **nota 5** t√™m, em m√©dia, pre√ßo de **R$ {preco_medio_5:,.2f}**.
- J√° os produtos com **nota 1** t√™m pre√ßo m√©dio de **R$ {preco_medio_1:,.2f}**.
- O produto com a **melhor avalia√ß√£o** possui nota **{preco_mais_bem_avaliado['media_avaliacao']:.1f}** e pre√ßo m√©dio de **R$ {preco_mais_bem_avaliado['preco']:,.2f}**.
- O produto com a **pior avalia√ß√£o** tem nota **{preco_mais_mal_avaliado['media_avaliacao']:.1f}** e custa em m√©dia **R$ {preco_mais_mal_avaliado['preco']:,.2f}**.
""")

st.divider()

#%%
from datetime import datetime
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import plotly.express as px


# 1. Filtrar avalia√ß√µes ruins (nota 1 ou 2)
clientes_ruins = dados['order_reviews'][dados['order_reviews']['review_score'] <= 2] \
    .merge(dados['orders'], on='order_id') \
    .merge(dados['order_items'], on='order_id') \
    .merge(dados['customers'], on='customer_id')  # Garantir que traga o customer_unique_id

# 2. Converter datas
clientes_ruins['order_purchase_timestamp'] = pd.to_datetime(clientes_ruins['order_purchase_timestamp'])

# 3. Refer√™ncia temporal
referencia = clientes_ruins['order_purchase_timestamp'].max()

# 4. Calcular m√©tricas RFM
df_rfm = clientes_ruins.groupby('customer_unique_id').agg({
    'order_purchase_timestamp': lambda x: (referencia - x.max()).days,
    'order_id': 'nunique',
    'price': 'sum'
}).reset_index().rename(columns={
    'order_purchase_timestamp': 'recencia',
    'order_id': 'frequencia',
    'price': 'monetario'
})

# 5. Normalizar
scaler = StandardScaler()
rfm_normalizado = scaler.fit_transform(df_rfm[['recencia', 'frequencia', 'monetario']])

# 6. KMeans
kmeans = KMeans(n_clusters=3, random_state=42, n_init='auto')
df_rfm['cluster'] = kmeans.fit_predict(rfm_normalizado)

# 7. Gr√°fico 3D
fig = px.scatter_3d(
    df_rfm,
    x='recencia',
    y='frequencia',
    z='monetario',
    color='cluster',
    title='<b>Clusteriza√ß√£o de Clientes Insatisfeitos (RFM)</b>',
    labels={'recencia': 'Rec√™ncia (dias)', 'frequencia': 'Frequ√™ncia', 'monetario': 'Gasto (R$)'},
    color_discrete_sequence=px.colors.qualitative.Set1
)

fig.update_layout(
    plot_bgcolor='rgba(0,0,0,0)',
    paper_bgcolor='rgba(0,0,0,0)',
    font=dict(color='white'),
    margin=dict(l=50, r=50, t=80, b=50),
    height=600
)

st.plotly_chart(fig, use_container_width=True)


# 1. Resumo estat√≠stico dos clusters
resumo_clusters = df_rfm.groupby('cluster').agg({
    'recencia': 'mean',
    'frequencia': 'mean',
    'monetario': 'mean',
    'customer_unique_id': 'count'
}).rename(columns={'customer_unique_id': 'qtde_clientes'}).round(2).reset_index()

# 2. Ordenar clusters por frequ√™ncia ou monet√°rio
resumo_clusters = resumo_clusters.sort_values(by='monetario', ascending=False)

# 3. Gerar texto de an√°lise
texto = "### An√°lise dos Clusters de Clientes Insatisfeitos (RFM)\n\n"

for _, row in resumo_clusters.iterrows():
    texto += (
        f"- **Cluster {int(row['cluster'])}**: possui **{int(row['qtde_clientes'])} clientes**.\n"
        f"  - Rec√™ncia m√©dia: **{row['recencia']:.1f} dias** desde a √∫ltima compra\n"
        f"  - Frequ√™ncia m√©dia: **{row['frequencia']:.1f} pedidos**\n"
        f"  - Gasto m√©dio: **R$ {row['monetario']:.2f}**\n\n"
    )

texto += "*Esses grupos ajudam a identificar quais clientes insatisfeitos compram com frequ√™ncia, gastam mais e podem ser priorit√°rios para a√ß√µes de recupera√ß√£o.*"
st.markdown(texto)

st.dataframe(resumo_clusters)

# Filtro para escolher o cluster
cluster_selecionado = st.selectbox(
    "Selecione o cluster para visualizar os clientes:",
    df_rfm['cluster'].sort_values().unique()
)

# Filtrar o DataFrame com base no cluster selecionado
clientes_cluster = df_rfm[df_rfm['cluster'] == cluster_selecionado]

# Exibir tabela com os dados dos clientes
st.write(f"Clientes no cluster {cluster_selecionado}:")
st.dataframe(
    clientes_cluster[['customer_unique_id', 'recencia', 'frequencia', 'monetario']]
    .sort_values(by='recencia')
    .reset_index(drop=True)
)

# T√≠tulo e descri√ß√£o
st.header('Clusteriza√ß√£o de Clientes Insatisfeitos')
st.write("""
Este sistema permite exportar os dados dos clientes de um cluster espec√≠fico para um arquivo CSV ou Excel. Voc√™ pode selecionar um cluster e gerar os arquivos para an√°lise.

Clique no bot√£o abaixo para exportar os dados dos clientes de um determinado cluster.
""")

# Op√ß√£o de selecionar o cluster
cluster_selecionado = st.selectbox(
    'Selecione o Cluster:',
    [0, 1, 2]
)

# Filtra os clientes do cluster selecionado
clientes_do_cluster = clientes_cluster[clientes_cluster['cluster'] == cluster_selecionado]

# Fun√ß√£o para exportar os dados
def exportar_dados(format='csv'):
    if format == 'csv':
        clientes_do_cluster.to_csv(f'clientes_cluster_{cluster_selecionado}.csv', index=False)
        st.success(f'Os dados do Cluster {cluster_selecionado} foram exportados para CSV!')
    elif format == 'excel':
        clientes_do_cluster.to_excel(f'clientes_cluster_{cluster_selecionado}.xlsx', index=False)
        st.success(f'Os dados do Cluster {cluster_selecionado} foram exportados para Excel!')

# Bot√µes para exportar os dados
col1, col2 = st.columns(2)

with col1:
    if st.button('Exportar para CSV'):
        exportar_dados(format='csv')

with col2:
    if st.button('Exportar para Excel'):
        exportar_dados(format='excel')

st.divider()

#%%
import json

st.header('Comportamento de Compra por Regi√£o')

@st.cache_data
def carregar_dados():
    data_path = "data/"
    customers = pd.read_csv(data_path + "olist_customers_dataset.csv", usecols=["customer_id", "customer_state"])
    order_items = pd.read_csv(data_path + "olist_order_items_dataset.csv", usecols=["order_id", "price"])
    orders = pd.read_csv(data_path + "olist_orders_dataset.csv", usecols=["order_id", "customer_id"])
    return customers, order_items, orders

def gerar_grafico_barras(customers, order_items, orders):
    vendas_por_estado = (
        orders
        .merge(order_items, on="order_id")
        .merge(customers, on="customer_id")
        .groupby("customer_state")["price"]
        .sum()
        .reset_index()
    )

    vendas_por_estado.columns = ["sigla", "vendas"]

    # Ordenando as vendas de menor para maior
    vendas_por_estado = vendas_por_estado.sort_values(by="vendas", ascending=True)

    fig = px.bar(
        vendas_por_estado,
        x="vendas",
        y="sigla",
        orientation="h",
        color="vendas",
        color_continuous_scale="Cividis",
        labels={"vendas": "Volume de Vendas (R$)", "sigla": "Estado"},
        title="Volume de Vendas por Estado",
        height=600
    )
    
    return fig, vendas_por_estado

# Carregar dados
customers, order_items, orders = carregar_dados()

# Gerar gr√°fico de barras e obter o dataframe de vendas por estado
fig, vendas_por_estado = gerar_grafico_barras(customers, order_items, orders)

# Exibir gr√°fico
st.plotly_chart(fig, use_container_width=True)

st.markdown("""
O gr√°fico a cima mostra o total de vendas realizadas por estado brasileiro, revelando o volume financeiro movimentado em cada unidade da federa√ß√£o.

### An√°lise de Comportamento de Compra por Regi√£o
- Estados como **Roraima**, **Amap√°** e **Acre** apresentam os **menores volumes de vendas** do pa√≠s.
- **S√£o Paulo** lidera com folga, somando **mais de R$ 5 milh√µes em vendas**.
- O valor movimentado por **SP √© mais que o dobro da soma dos 10 estados com menor volume**.
- Esses dados evidenciam a **forte concentra√ß√£o de consumo no Sudeste**, especialmente em S√£o Paulo.
- H√° **potenciais oportunidades de crescimento** em regi√µes com menor movimenta√ß√£o, que podem ser exploradas com a√ß√µes direcionadas de marketing e log√≠stica.
""")

# Exibir a tabela de vendas por estado ap√≥s o gr√°fico
st.markdown("""
#### Tabela de Vendas por Estado
Abaixo est√° a tabela com o total de vendas realizadas em cada estado, que tamb√©m pode ser √∫til para an√°lises mais detalhadas.
""")

# Exibir a tabela de vendas
st.dataframe(vendas_por_estado, height=200)

st.divider()

#%%

# Dicion√°rio de estados por regi√£o
estados_por_regiao = {
    "Norte": ["AC", "AP", "AM", "PA", "RO", "RR", "TO"],
    "Nordeste": ["AL", "BA", "CE", "MA", "PB", "PE", "PI", "RN", "SE"],
    "Centro-Oeste": ["DF", "GO", "MT", "MS"],
    "Sudeste": ["ES", "MG", "RJ", "SP"],
    "Sul": ["PR", "RS", "SC"]
}

# Inverter para mapear estado -> regi√£o
estado_para_regiao = {estado: regiao for regiao, estados in estados_por_regiao.items() for estado in estados}

@st.cache_data
def carregar_dados():
    data_path = "data/"
    customers = pd.read_csv(data_path + "olist_customers_dataset.csv", usecols=["customer_id", "customer_state"])
    order_items = pd.read_csv(data_path + "olist_order_items_dataset.csv", usecols=["order_id", "price"])
    orders = pd.read_csv(data_path + "olist_orders_dataset.csv", usecols=["order_id", "customer_id"])
    return customers, order_items, orders

def calcular_ticket_medio_por_regiao(customers, order_items, orders):
    df = (
        orders
        .merge(order_items, on="order_id")
        .merge(customers, on="customer_id")
    )

    df["regiao"] = df["customer_state"].map(estado_para_regiao)

    ticket_medio = df.groupby("regiao").agg(
        total_vendas=("price", "sum"),
        total_pedidos=("order_id", "nunique")
    ).reset_index()

    ticket_medio["ticket_medio"] = ticket_medio["total_vendas"] / ticket_medio["total_pedidos"]
    ticket_medio = ticket_medio.sort_values(by="ticket_medio", ascending=True)
    return ticket_medio

# Carregar dados
customers, order_items, orders = carregar_dados()

# Calcular ticket m√©dio
ticket_medio_df = calcular_ticket_medio_por_regiao(customers, order_items, orders)


# Gr√°fico
fig = px.bar(
    ticket_medio_df,
    x="ticket_medio",
    y="regiao",
    orientation="h",
    text_auto=".2s",
    labels={"ticket_medio": "Ticket M√©dio (R$)", "regiao": "Regi√£o"},
    color="ticket_medio",
    color_continuous_scale="Magma",
    title="Ticket M√©dio por Regi√£o",
    height=500
)

st.plotly_chart(fig, use_container_width=True)

# Texto introdut√≥rio e insight
st.markdown("""
O gr√°fico mostra o **ticket m√©dio por regi√£o**, ou seja, quanto em m√©dia os clientes gastam por pedido em cada parte do Brasil.

### An√°lise de Ticket M√©dio por Regi√£o
- Apesar de **Sudeste** e **Sul** liderarem em **volume total de vendas**, eles **n√£o possuem os maiores tickets m√©dios**.
- As regi√µes **Norte** e **Nordeste** apresentam os **maiores tickets m√©dios do pa√≠s**.
- Isso indica que, embora a **frequ√™ncia de compras** nessas regi√µes possa ser menor, os **valores por transa√ß√£o s√£o mais elevados**.
- Pode haver oportunidades para **explorar a recorr√™ncia de compra** nessas regi√µes com alto ticket.
""")

st.write('Abaixo temos uma tabela com maior detalhe, contendo todas as Regi√µes, Total de Vendas, Total de Pedidos e o Ticket M√©dio.')
# Exibir tabela
st.dataframe(ticket_medio_df, use_container_width=True)

st.divider()

#%%

@st.cache_data
def carregar_dados_categoria():
    data_path = "data/"
    orders = pd.read_csv(data_path + "olist_orders_dataset.csv", usecols=["order_id", "customer_id"])
    order_items = pd.read_csv(data_path + "olist_order_items_dataset.csv", usecols=["order_id", "product_id"])
    customers = pd.read_csv(data_path + "olist_customers_dataset.csv", usecols=["customer_id", "customer_state"])
    products = pd.read_csv(data_path + "olist_products_dataset.csv", usecols=["product_id", "product_category_name"])
    return orders, order_items, customers, products

def mapear_estado_para_regiao(estado):
    regioes = {
        "Norte": ["AC", "AP", "AM", "PA", "RO", "RR", "TO"],
        "Nordeste": ["AL", "BA", "CE", "MA", "PB", "PE", "PI", "RN", "SE"],
        "Centro-Oeste": ["DF", "GO", "MT", "MS"],
        "Sudeste": ["ES", "MG", "RJ", "SP"],
        "Sul": ["PR", "RS", "SC"],
    }
    for regiao, estados in regioes.items():
        if estado in estados:
            return regiao
    return "Outro"

def categoria_mais_vendida_por_regiao(orders, order_items, customers, products):
    df = (
        orders.merge(order_items, on="order_id")
              .merge(customers, on="customer_id")
              .merge(products, on="product_id")
    )
    df["regiao"] = df["customer_state"].apply(mapear_estado_para_regiao)
    vendas_por_categoria = (
        df.groupby(["regiao", "product_category_name"])
          .size()
          .reset_index(name="qtd_vendas")
    )
    mais_vendidas = (
        vendas_por_categoria.sort_values(["regiao", "qtd_vendas"], ascending=[True, False])
                            .groupby("regiao")
                            .first()
                            .reset_index()
    )
    return mais_vendidas

# Carregar dados
orders, order_items, customers, products = carregar_dados_categoria()

# Obter categorias mais vendidas por regi√£o
mais_vendidas_regiao = categoria_mais_vendida_por_regiao(orders, order_items, customers, products)

# Criar gr√°fico de barras
fig = px.bar(
    mais_vendidas_regiao,
    x="regiao",
    y="qtd_vendas",
    color="regiao",
    hover_data=["product_category_name"],
    labels={"qtd_vendas": "Quantidade de Vendas", "regiao": "Regi√£o", "product_category_name": "Categoria de Produto"},
    title="Categorias de Produtos Mais Vendidas por Regi√£o",
    height=600
)

# Exibir gr√°fico
st.plotly_chart(fig, use_container_width=True)

# Tabela
st.dataframe(mais_vendidas_regiao)

# Texto e an√°lise
st.markdown("""
### An√°lise das Categorias por Regi√£o
- Produtos como `beleza_saude`, `cama_mesa_banho` e `moveis_decoracao` se destacam em algumas regi√µes.
- A regi√£o **Sudeste** apresenta um volume significativo de vendas na categoria **cama_mesa_banho**, com mais de 8.000 unidades, o que reflete uma maior demanda por esses produtos.
- **Norte** e **Centro-Oeste** t√™m categorias como **beleza_saude** com volumes menores, o que pode indicar mercados menos saturados para essas categorias nessas regi√µes.
""")

st.divider()
#%%

st.header('Efici√™ncia da Log√≠stica (Entregas e Prazos)')

@st.cache_data
def carregar_dados_entrega():
    data_path = "data/"
    orders = pd.read_csv(data_path + "olist_orders_dataset.csv", parse_dates=["order_purchase_timestamp", "order_delivered_customer_date"])
    customers = pd.read_csv(data_path + "olist_customers_dataset.csv", usecols=["customer_id", "customer_state"])
    return orders, customers

def calcular_tempo_entrega(orders, customers):
    dados = (
        orders.dropna(subset=["order_delivered_customer_date"])
              .merge(customers, on="customer_id")
    )
    dados["tempo_entrega"] = (dados["order_delivered_customer_date"] - dados["order_purchase_timestamp"]).dt.days
    entrega_por_estado = dados.groupby("customer_state")["tempo_entrega"].mean().reset_index()
    entrega_por_estado = entrega_por_estado.sort_values(by="tempo_entrega", ascending=False)
    return entrega_por_estado

def plotar_tempo_entrega(df):
    fig = px.bar(
        df,
        x="tempo_entrega",
        y="customer_state",
        orientation="h",
        title="Tempo M√©dio de Entrega por Estado",
        labels={"tempo_entrega": "Dias", "customer_state": "Estado"},
        color="tempo_entrega",
        color_continuous_scale="Viridis",
        height=600
    )
    return fig

# Carregar e processar dados
orders, customers = carregar_dados_entrega()
entrega_por_estado = calcular_tempo_entrega(orders, customers)

# Mostrar gr√°fico
st.plotly_chart(plotar_tempo_entrega(entrega_por_estado), use_container_width=True)

# Tabela
st.dataframe(entrega_por_estado, height=200)

# Texto e insight
st.markdown("""
O gr√°fico a mostra o **tempo m√©dio (em dias) que os pedidos levaram para serem entregues em cada estado do Brasil**.

**An√°lise de Tempo M√©dio de Entrega por Localidade:**
- Estados da regi√£o **Norte** e **Centro-Oeste** geralmente apresentam **tempos m√©dios de entrega mais longos**, sugerindo poss√≠veis desafios log√≠sticos ou maior dist√¢ncia dos centros de distribui√ß√£o.
- Em contraste, regi√µes como **Sudeste** e **Sul** possuem **entregas mais r√°pidas**, reflexo de melhor infraestrutura, maior densidade urbana e proximidade com polos log√≠sticos.
- Esses dados indicam oportunidades de melhoria log√≠stica em √°reas menos favorecidas, que podem impactar positivamente a satisfa√ß√£o do cliente e os prazos de entrega.
""")

st.divider()


#%%

@st.cache_data
def carregar_dados_atrasos():
    data_path = "data/"
    orders = pd.read_csv(
        data_path + "olist_orders_dataset.csv",
        parse_dates=["order_delivered_customer_date", "order_estimated_delivery_date"]
    )
    customers = pd.read_csv(
        data_path + "olist_customers_dataset.csv",
        usecols=["customer_id", "customer_state"]
    )
    return orders, customers

def calcular_atrasos_reais(orders, customers):
    dados = (
        orders.dropna(subset=["order_delivered_customer_date", "order_estimated_delivery_date"])
              .merge(customers, on="customer_id")
    )
    dados["dias_atraso"] = (dados["order_delivered_customer_date"] - dados["order_estimated_delivery_date"]).dt.days
    dados = dados[dados["dias_atraso"] > 0]  # Considera apenas entregas com atraso
    atraso_por_estado = dados.groupby("customer_state")["dias_atraso"].mean().reset_index()
    atraso_por_estado = atraso_por_estado.sort_values(by="dias_atraso", ascending=False)
    return atraso_por_estado

def plotar_atrasos(df):
    fig = px.bar(
        df,
        x="dias_atraso",
        y="customer_state",
        orientation="h",
        title="Tempo M√©dio de Atraso por Estado (Real x Estimado)",
        labels={"dias_atraso": "Dias de Atraso", "customer_state": "Estado"},
        color="dias_atraso",
        color_continuous_scale="Rdylbu",
        height=600
    )
    return fig

# Execu√ß√£o no Streamlit
orders, customers = carregar_dados_atrasos()
df_atrasos = calcular_atrasos_reais(orders, customers)

st.plotly_chart(plotar_atrasos(df_atrasos), use_container_width=True)
st.dataframe(df_atrasos, height=200)

# Texto explicativo
st.markdown("""
O gr√°fico acima apresenta o **tempo m√©dio de atraso na entrega** dos pedidos em cada estado do Brasil, comparando a data de entrega real com a data estimada originalmente.

### An√°lise de Atrasos Reais nas Entregas
- Estados mais distantes dos grandes centros, como os da regi√£o **Norte**, tendem a apresentar os **maiores atrasos m√©dios**.
- J√° os estados do **Sudeste** e **Sul**, com maior infraestrutura e proximidade de centros log√≠sticos, mostram **menores √≠ndices de atraso**.
- Entender esses gargalos √© fundamental para redesenhar rotas e melhorar a experi√™ncia de entrega.
""")

st.divider()

#%%
@st.cache_data
def carregar_dados_rfm():
    path = "data/"
    orders = pd.read_csv(path + "olist_orders_dataset.csv", parse_dates=["order_purchase_timestamp"])
    order_items = pd.read_csv(path + "olist_order_items_dataset.csv", usecols=["order_id", "price"])
    return orders, order_items

def calcular_rfm(orders, order_items):
    # Apenas pedidos entregues
    pedidos = orders[orders["order_status"] == "delivered"].copy()

    # Combina pedidos com valores
    dados = pedidos.merge(order_items, on="order_id")

    # Data de refer√™ncia para c√°lculo da rec√™ncia
    data_ref = dados["order_purchase_timestamp"].max()

    # Agrupamento por cliente
    rfm = dados.groupby("customer_id").agg({
        "order_purchase_timestamp": lambda x: (data_ref - x.max()).days,  # Rec√™ncia
        "order_id": "nunique",  # Frequ√™ncia
        "price": "sum"  # Valor monet√°rio
    }).reset_index()

    rfm.columns = ["customer_id", "Recencia", "Frequencia", "Valor"]

    # Score RFM (dividido em 4 quantis)
    rfm["R_quartil"] = pd.qcut(rfm["Recencia"], 4, labels=[4, 3, 2, 1])
    rfm["F_quartil"] = pd.qcut(rfm["Frequencia"].rank(method="first"), 4, labels=[1, 2, 3, 4])
    rfm["M_quartil"] = pd.qcut(rfm["Valor"], 4, labels=[1, 2, 3, 4])

    # Segmento RFM
    rfm["RFM_Score"] = rfm["R_quartil"].astype(str) + rfm["F_quartil"].astype(str) + rfm["M_quartil"].astype(str)

    # Segmentos
    def segmentar_cliente(r, f, m):
        if r >= 3 and f >= 3 and m >= 3:
            return "Cliente Premium"
        elif r >= 3 and f >= 2:
            return "Leal"
        elif r <= 2 and f <= 2:
            return "Em Risco"
        elif f >= 3:
            return "Potencial"
        else:
            return "Outros"

    rfm["Segmento"] = rfm[["R_quartil", "F_quartil", "M_quartil"]].apply(
        lambda x: segmentar_cliente(int(x[0]), int(x[1]), int(x[2])), axis=1
    )

    return rfm

orders, order_items = carregar_dados_rfm()
rfm_df = calcular_rfm(orders, order_items)

st.header("An√°lise de RFM (Rec√™ncia, Frequ√™ncia e Valor Monet√°rio)")

segmentos = rfm_df["Segmento"].value_counts().reset_index()
segmentos.columns = ["Segmento", "count"]

fig = px.bar(
    segmentos,
    x="Segmento",
    y="count",
    labels={"Segmento": "Segmento", "count": "N√∫mero de Clientes"},
    title="Distribui√ß√£o dos Segmentos de Clientes",
    color="Segmento",
    color_discrete_sequence=px.colors.qualitative.Set3
)


st.plotly_chart(fig, use_container_width=True)

st.markdown("""
### An√°lise por Segmento de Clientes (RFM)

A segmenta√ß√£o RFM classifica os clientes em grupos com base em tr√™s fatores: **Rec√™ncia**, **Frequ√™ncia** e **Valor Monet√°rio**. Aqui est√£o os principais grupos:

- **Clientes Premium**: Compram frequentemente, gastam alto e est√£o ativos recentemente. Devem ser priorizados com ofertas exclusivas.
  
- **Clientes Leais**: Compram com frequ√™ncia e t√™m alto gasto, mas podem estar inativos. A reativa√ß√£o com campanhas estrat√©gicas √© ideal.

- **Clientes em Potencial**: Clientes com bom valor ou frequ√™ncia, mas inativos. A reativa√ß√£o pode ser feita com promo√ß√µes e produtos personalizados.

- **Clientes em Risco**: Compraram muito, mas est√£o inativos. S√£o ideais para campanhas de recupera√ß√£o.

- **Clientes Rec√©m-Chegados**: Compraram recentemente, mas com baixo valor e frequ√™ncia. Reforce a experi√™ncia e incentive a recompra.

- **Clientes Inativos**: N√£o compram h√° algum tempo. Avalie a viabilidade de reengaj√°-los com campanhas espec√≠ficas.

Esta segmenta√ß√£o permite estrat√©gias direcionadas e eficientes para reten√ß√£o e crescimento.
""")

st.divider()


#%%

st.header("Conclus√£o")

st.markdown("""
Este projeto teve como objetivo explorar dados de vendas e log√≠stica da **Olist**, com o intuito de identificar padr√µes de comportamento de clientes, avaliar a efici√™ncia log√≠stica e sugerir estrat√©gias para melhorar a performance da empresa.

### Principais an√°lises realizadas:
- **Atrasos na entrega x avalia√ß√£o dos clientes**: revelou que a pontualidade √© um fator crucial na experi√™ncia do consumidor.
- **Comportamento de compra por regi√£o**: identificou varia√ß√µes no volume e ticket m√©dio, sugerindo estrat√©gias de marketing segmentadas.
- **Segmenta√ß√£o de clientes via RFM (Rec√™ncia, Frequ√™ncia, Valor Monet√°rio)**: permitiu identificar clientes valiosos e os com maior risco de churn.

### Recomenda√ß√µes para a Olist:
- Melhorar a log√≠stica nas regi√µes com maiores atrasos (ex: Norte e Centro-Oeste) para reduzir o tempo de entrega e aumentar a satisfa√ß√£o dos clientes.
- Criar campanhas espec√≠ficas para regi√µes com maior ticket m√©dio (como Norte e Nordeste), incentivando maior frequ√™ncia de compra.
- Implementar estrat√©gias de reten√ß√£o para grupos de clientes "em risco", com promo√ß√µes e vantagens exclusivas.

### Limita√ß√µes observadas:
- Falta de dados sobre transportadoras.
- Aus√™ncia de informa√ß√µes mais granulares sobre o comportamento de navega√ß√£o dos clientes.

No futuro, seria interessante incluir dados de comportamento online e feedback direto dos consumidores para expandir ainda mais a an√°lise e prever tend√™ncias futuras de vendas.

---

### Conclus√£o final:
Este projeto oferece uma vis√£o detalhada sobre o comportamento do cliente e a performance log√≠stica da Olist, servindo como base s√≥lida para **decis√µes mais informadas e estrat√©gias de melhoria cont√≠nua**.
""")


#%%
st.markdown("---")
st.markdown("""
<div style='text-align: center; font-size: 0.9em; color: gray;'>
    Desenvolvido por <strong>Adriano Mayco</strong> ‚Ä¢ 
    <a href='https://www.linkedin.com/in/adriano-mayco-382256221/' target='_blank'>LinkedIn</a> ‚Ä¢ 
    <a href='https://github.com/AdriMayc' target='_blank'>GitHub</a> ‚Ä¢ 
    <a href='https://github.com/AdriMayc/Analise_de_Dados.git' target='_blank'>Reposit√≥rio do Projeto</a><br>
    Maio de 2025 ‚Ä¢ Vers√£o 1.0
</div>
""", unsafe_allow_html=True)

